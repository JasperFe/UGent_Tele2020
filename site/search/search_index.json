{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Teledetectie Practica 2020 \u00b6 Welkom bij de cursussite voor de practica van Teledetectie 2020. Doorheen de practica wordt deze site aangevuld met nieuwe documentatie, extra informatie en FAQ's. Remote Sensing | Spatial Analysis lab (REMOSA)","title":"Home"},{"location":"#teledetectie-practica-2020","text":"Welkom bij de cursussite voor de practica van Teledetectie 2020. Doorheen de practica wordt deze site aangevuld met nieuwe documentatie, extra informatie en FAQ's. Remote Sensing | Spatial Analysis lab (REMOSA)","title":"Teledetectie Practica 2020"},{"location":"P3/P3-Sentinel2/","text":"The ESA Copernicus programme \u00b6 Copernicus is the EU's Earth Observation Programme, looking at our planet and its environment for the ultimate benefit of all European citizens. The overall goal is achieving a global, continuous, autonomous, high quality, wide range Earth observation capacity. Under the copernicus programme, ESA is developing a series of next-generation Earth observation missions under the name of 'Sentinel' programme. This Sentinel Programme, consists of multiple satellites, each focussing on a different aspect of Earth observation: atmospheric, Oceanic and Land monitoring: Current Sentinel satellites, with their main goal. (Source: ESA) In this practical will focus on the multispectral imagery taken by Sentinel 2 satellites. The Sentinel-2 mission \u00b6 Sentinel-2 is the copernicus Earth observation mission by ESA with the goal to perform terrestrial observations in support of services such as forest monitoring, land cover changes detection, and natural disaster management. It consists of two identical satellites, Sentinel-2A and Sentinel-2B. An interesting infograph about the Sentinel-2 mission can be found here . The Sentinel-2 mission has the following capabilities: Multi-spectral data with 13 bands in the visible, near infrared, and short wave infrared part of the spectrum Systematic global coverage of land surfaces from 56\u00b0 S to 84\u00b0 N, coastal waters, and all of the Mediterranean Sea Revisiting every 5 days under the same viewing angles. Spatial resolution of 10 m, 20 m and 60 m 290 km field of view Free and open data policy To achieve frequent revisits and high mission availability, the two identical Sentinel-2 satellites (Sentinel-2A and Sentinel-2B) operate simultaneously. The orbit is Sun synchronous at 786 km (488 mi) altitude. Sentinel 2 data download \u00b6 All data captured by the ESA copernicus Sentinel program are completely freely available to the public. The most convinient way to download Sentinel data is through the Copernicus Open Access Hub, a platform dedicate to provide easy acces to the user. For this, an user account is required. To register go to registration page . To acces the data hub, go to https://scihub.copernicus.eu/ . Ex 3.1 - Downloading a Sentinel 2 Level 1C image In the first exercise, you will download an image from the Copernicus Open Access Hub. Go to https://scihub.copernicus.eu/ Klick \u2018Open hub\u2019 to access the Interactive Graphical User Interface Log in (or create an account) Zoom to Bel\u00e8m, a city in the north of Brazil, close to the gateway of the Amazon river Switch the \u2018Open street\u2019 view to \u2018sentinel-2 cloudless + Overlay\u2019 view Switch to \u2018navigation mode\u2019 Draw a rectangle around Bel\u00e8m: At the button \u2018Insert search criteria\u2019: go for \u2018advanced search\u2019 Look for a 2020 image (sensing period), Sentinel-2A, level 1C (product type) with a cloud cover of maximum 10%. Then click on the search button: Click on the search button Search for an image that contains the major part of the city (inspect the image in a quick look view ) Download this image to a folder on your computer. Sentinel file naming convention \u00b6 The naming of the Sentinel products follows the Compact Naming Convention: MMM_MSIXXX_YYYYMMDDHHMMSS_Nxxyy_ROOO_Txxxxx_\"Product Discriminator\".SAFE Where: MMM : is the mission ID (S2A/S2B) MSIXXX : MSIL1C denotes the Level-1C product level/ MSIL2A denotes the Level-2A product level (see \u2018radiometric correction\u2019). YYYYMMDDTHHMMSS : the datatake sensing start time Nxxyy : the Processing Baseline number (e.g. N0204) ROOO : Relative Orbit number (R001 - R143) Txxxxx : Tile Number field .SAFE : Product Format (Standard Archive Format for Europe) The products contain two dates. The first date (YYYYMMDDHHMMSS) is the datatake sensing time. The second date is the \"Product Discriminator\" field, which is 15 characters in length, and is used to distinguish between different end user products from the same datatake. Depending on the instance, the time in this field can be earlier or slightly later than the datatake sensing time. Thus, the following filename \u2018S2A_MSIL1C_20170105T013442_N0204_R031_T53NMJ_20170105T013443.SAFE\u2019 identifies a Level-1C product acquired by Sentinel-2A on the 5th of January, 2017 at 1:34:42 AM. It was acquired over Tile 53NMJ(2) during Relative Orbit 031, and processed with PDGS Processing Baseline 02.04. Ex 3.2 - naming convention Explain the different components of the name: S2A_MSIL1C_20180812T143751_N0206_R096_T19KGA_20180812T182110 (example)","title":"Sentinel-2 intro and download"},{"location":"P3/P3-Sentinel2/#the-esa-copernicus-programme","text":"Copernicus is the EU's Earth Observation Programme, looking at our planet and its environment for the ultimate benefit of all European citizens. The overall goal is achieving a global, continuous, autonomous, high quality, wide range Earth observation capacity. Under the copernicus programme, ESA is developing a series of next-generation Earth observation missions under the name of 'Sentinel' programme. This Sentinel Programme, consists of multiple satellites, each focussing on a different aspect of Earth observation: atmospheric, Oceanic and Land monitoring: Current Sentinel satellites, with their main goal. (Source: ESA) In this practical will focus on the multispectral imagery taken by Sentinel 2 satellites.","title":"The ESA Copernicus programme"},{"location":"P3/P3-Sentinel2/#the-sentinel-2-mission","text":"Sentinel-2 is the copernicus Earth observation mission by ESA with the goal to perform terrestrial observations in support of services such as forest monitoring, land cover changes detection, and natural disaster management. It consists of two identical satellites, Sentinel-2A and Sentinel-2B. An interesting infograph about the Sentinel-2 mission can be found here . The Sentinel-2 mission has the following capabilities: Multi-spectral data with 13 bands in the visible, near infrared, and short wave infrared part of the spectrum Systematic global coverage of land surfaces from 56\u00b0 S to 84\u00b0 N, coastal waters, and all of the Mediterranean Sea Revisiting every 5 days under the same viewing angles. Spatial resolution of 10 m, 20 m and 60 m 290 km field of view Free and open data policy To achieve frequent revisits and high mission availability, the two identical Sentinel-2 satellites (Sentinel-2A and Sentinel-2B) operate simultaneously. The orbit is Sun synchronous at 786 km (488 mi) altitude.","title":"The Sentinel-2 mission"},{"location":"P3/P3-Sentinel2/#sentinel-2-data-download","text":"All data captured by the ESA copernicus Sentinel program are completely freely available to the public. The most convinient way to download Sentinel data is through the Copernicus Open Access Hub, a platform dedicate to provide easy acces to the user. For this, an user account is required. To register go to registration page . To acces the data hub, go to https://scihub.copernicus.eu/ . Ex 3.1 - Downloading a Sentinel 2 Level 1C image In the first exercise, you will download an image from the Copernicus Open Access Hub. Go to https://scihub.copernicus.eu/ Klick \u2018Open hub\u2019 to access the Interactive Graphical User Interface Log in (or create an account) Zoom to Bel\u00e8m, a city in the north of Brazil, close to the gateway of the Amazon river Switch the \u2018Open street\u2019 view to \u2018sentinel-2 cloudless + Overlay\u2019 view Switch to \u2018navigation mode\u2019 Draw a rectangle around Bel\u00e8m: At the button \u2018Insert search criteria\u2019: go for \u2018advanced search\u2019 Look for a 2020 image (sensing period), Sentinel-2A, level 1C (product type) with a cloud cover of maximum 10%. Then click on the search button: Click on the search button Search for an image that contains the major part of the city (inspect the image in a quick look view ) Download this image to a folder on your computer.","title":"Sentinel 2 data download"},{"location":"P3/P3-Sentinel2/#sentinel-file-naming-convention","text":"The naming of the Sentinel products follows the Compact Naming Convention: MMM_MSIXXX_YYYYMMDDHHMMSS_Nxxyy_ROOO_Txxxxx_\"Product Discriminator\".SAFE Where: MMM : is the mission ID (S2A/S2B) MSIXXX : MSIL1C denotes the Level-1C product level/ MSIL2A denotes the Level-2A product level (see \u2018radiometric correction\u2019). YYYYMMDDTHHMMSS : the datatake sensing start time Nxxyy : the Processing Baseline number (e.g. N0204) ROOO : Relative Orbit number (R001 - R143) Txxxxx : Tile Number field .SAFE : Product Format (Standard Archive Format for Europe) The products contain two dates. The first date (YYYYMMDDHHMMSS) is the datatake sensing time. The second date is the \"Product Discriminator\" field, which is 15 characters in length, and is used to distinguish between different end user products from the same datatake. Depending on the instance, the time in this field can be earlier or slightly later than the datatake sensing time. Thus, the following filename \u2018S2A_MSIL1C_20170105T013442_N0204_R031_T53NMJ_20170105T013443.SAFE\u2019 identifies a Level-1C product acquired by Sentinel-2A on the 5th of January, 2017 at 1:34:42 AM. It was acquired over Tile 53NMJ(2) during Relative Orbit 031, and processed with PDGS Processing Baseline 02.04. Ex 3.2 - naming convention Explain the different components of the name: S2A_MSIL1C_20180812T143751_N0206_R096_T19KGA_20180812T182110 (example)","title":"Sentinel file naming convention"},{"location":"P3/P3-Snap-intro/","text":"About SNAP \u00b6 SNAP, the SeNtinel Application Platform is developed by the ESA specifically to process Sentinel-imagery, however also other remotey sensed images can be read. The current version is 8.0.0 . SNAP is a relatively new software especially designed for the analysis of Sentinel products (Sentinel 2A was launched in 2015) and hence still contains some bugs (especially for mac-users, might try the older version 7.0.0). Not all applications are supported that you will find in classic Image Processing programs such as ENVI, but it is very user friendly and ideal to introduce you to satellite image processing. Also, it is free! Overview \u00b6 We will use SNAP to examine some image composites, and necessary preprocessing steps. After that, we will do most other processing with Google Earth Engine. Excercise: Opening a Sentinel-2 image in snap Open the sentinel image that you have downloaded (you do not need to unzip it). You can do this in several ways: Drag and drop the zip folder in the Products explorer Click file > Open Products and browse to your zip-folder Click and browse to your zip-folder. Unfold the image folder. Explore the files included. Open the Blue, Green, Red and NIR image. Test the tile buttons. Make sure you can see the four images simultaneously: Explore the navigation panel. Sentinel 2 Bands Let's have a quick look at the specifications of a Sentinel-2 image. There are 13 Sentinel 2 bands in total, with a resolution of 10, 20 or 60m: The navigation window \u00b6 The Navigation Window is used to move the viewport of an Image View, to zoom in and out of it and to rotate the image in steps of 5 degrees using the spinner control below the image preview. The current viewport is depicted by a semi-transparent rectangle, which can be dragged in order to move the viewport to another location. The navigation window In the bottom left, you will find the zoom factor: zoom is relative to the drawing extents. A scale factor of: 1 shows a part of the image 2 shows entities twice as large 0.5 shows entities half as large The text box at the left side of slider can be used to adjust the zoom factor manually. The Navigation window additionally provides the following features via its tool buttons (top right): Zoom In : Zooms in by a factor of 1.2. Zoom Out : Zooms out by a factor of 1/1.2. Zoom Actual Pixel : Sets the zoom factor to the default value so that the size of an image pixel has the same size of a display pixel. Zoom All : Adjusts the viewport to cover the entire image. Synchronise Views : Synchronises the viewports of all compatible image views. Synchronise Cursor : Displays a synchronised cursor on all opened image views. You can also zoom the images by scrolling on the image, or by clicking in the toolbar. Zoom factor vs Representative Fraction The zoom factor is not the same as a Representative Fraction (RF) , which is often used to indicate the scale of a map. The RF indicates the ratio between the number of units on the map to the number of units on the ground. The RF factor 1:100000 e.g. implies that one cm on map is equal to 1 km on land. Maps are described as either large-scale or small-scale. Large-scale maps show a smaller amount of area with a greater amount of detail. The geographic extent shown on a large-scale map is small. A large scaled map expressed as a representative scale would have a smaller number to the right of the ratio. For example, a large-scale map could have a RF scale of 1: 1,000. Large-scale maps are typically used to show neighbourhoods, a localize area, small towns, etc. Small-scale maps show a larger geographic area with few details on them. The RF scale of a small-scale map would have a much larger number to the right of the colon such as 1: 1,000,000. Small-scale maps are used to show the extent of an entire country, region, or continent. Zoom to the Airport Explore the World View panel . The red rectangle indicates the position of the image on the globe. The Colour Manipulation tool \u00b6 The colour manipulation tool window window is used to modify the colours used in the image. If you are opening an Image View of a data product's band, the Sentinel Toolbox either loads image settings from the product itself (BEAM-DIMAP format only) or uses default colour settings. In the Colour manipulation panel, explore the histogram. On the image, zoom to the airport and adjust the contrast. Restore the contrast afterwards. Pixel info view \u00b6 If you click on the tab 'Pixel View' (right to the product explorer), pixel information will be displayed while you move the mouse over the band image view.","title":"Introduction to SNAP"},{"location":"P3/P3-Snap-intro/#about-snap","text":"SNAP, the SeNtinel Application Platform is developed by the ESA specifically to process Sentinel-imagery, however also other remotey sensed images can be read. The current version is 8.0.0 . SNAP is a relatively new software especially designed for the analysis of Sentinel products (Sentinel 2A was launched in 2015) and hence still contains some bugs (especially for mac-users, might try the older version 7.0.0). Not all applications are supported that you will find in classic Image Processing programs such as ENVI, but it is very user friendly and ideal to introduce you to satellite image processing. Also, it is free!","title":"About SNAP"},{"location":"P3/P3-Snap-intro/#overview","text":"We will use SNAP to examine some image composites, and necessary preprocessing steps. After that, we will do most other processing with Google Earth Engine. Excercise: Opening a Sentinel-2 image in snap Open the sentinel image that you have downloaded (you do not need to unzip it). You can do this in several ways: Drag and drop the zip folder in the Products explorer Click file > Open Products and browse to your zip-folder Click and browse to your zip-folder. Unfold the image folder. Explore the files included. Open the Blue, Green, Red and NIR image. Test the tile buttons. Make sure you can see the four images simultaneously: Explore the navigation panel. Sentinel 2 Bands Let's have a quick look at the specifications of a Sentinel-2 image. There are 13 Sentinel 2 bands in total, with a resolution of 10, 20 or 60m:","title":"Overview"},{"location":"P3/P3-Snap-intro/#the-navigation-window","text":"The Navigation Window is used to move the viewport of an Image View, to zoom in and out of it and to rotate the image in steps of 5 degrees using the spinner control below the image preview. The current viewport is depicted by a semi-transparent rectangle, which can be dragged in order to move the viewport to another location. The navigation window In the bottom left, you will find the zoom factor: zoom is relative to the drawing extents. A scale factor of: 1 shows a part of the image 2 shows entities twice as large 0.5 shows entities half as large The text box at the left side of slider can be used to adjust the zoom factor manually. The Navigation window additionally provides the following features via its tool buttons (top right): Zoom In : Zooms in by a factor of 1.2. Zoom Out : Zooms out by a factor of 1/1.2. Zoom Actual Pixel : Sets the zoom factor to the default value so that the size of an image pixel has the same size of a display pixel. Zoom All : Adjusts the viewport to cover the entire image. Synchronise Views : Synchronises the viewports of all compatible image views. Synchronise Cursor : Displays a synchronised cursor on all opened image views. You can also zoom the images by scrolling on the image, or by clicking in the toolbar. Zoom factor vs Representative Fraction The zoom factor is not the same as a Representative Fraction (RF) , which is often used to indicate the scale of a map. The RF indicates the ratio between the number of units on the map to the number of units on the ground. The RF factor 1:100000 e.g. implies that one cm on map is equal to 1 km on land. Maps are described as either large-scale or small-scale. Large-scale maps show a smaller amount of area with a greater amount of detail. The geographic extent shown on a large-scale map is small. A large scaled map expressed as a representative scale would have a smaller number to the right of the ratio. For example, a large-scale map could have a RF scale of 1: 1,000. Large-scale maps are typically used to show neighbourhoods, a localize area, small towns, etc. Small-scale maps show a larger geographic area with few details on them. The RF scale of a small-scale map would have a much larger number to the right of the colon such as 1: 1,000,000. Small-scale maps are used to show the extent of an entire country, region, or continent. Zoom to the Airport Explore the World View panel . The red rectangle indicates the position of the image on the globe.","title":"The navigation window"},{"location":"P3/P3-Snap-intro/#the-colour-manipulation-tool","text":"The colour manipulation tool window window is used to modify the colours used in the image. If you are opening an Image View of a data product's band, the Sentinel Toolbox either loads image settings from the product itself (BEAM-DIMAP format only) or uses default colour settings. In the Colour manipulation panel, explore the histogram. On the image, zoom to the airport and adjust the contrast. Restore the contrast afterwards.","title":"The Colour Manipulation tool"},{"location":"P3/P3-Snap-intro/#pixel-info-view","text":"If you click on the tab 'Pixel View' (right to the product explorer), pixel information will be displayed while you move the mouse over the band image view.","title":"Pixel info view"},{"location":"P3/P3-colour-composites-ctd/","text":"Displaying more band combinations \u00b6 When you have performed an image resampling, open again the RGB image window in SNAP. You will notice that the list with possible band combinations is larger. Test some of the following band combinations and explore the colour differences. Which features are most clear on the following band combinations? Natural Colours: 4 3 2 False colour Infrared: 8 4 3 False colour Urban: 12 11 4 Agriculture: 11 8 2 Atmospheric penetration: 12 11 8a Healthy vegetation: 8 11 2 Land/Water: 8 11 4 Natural Colours with Atmospheric Removal: 12 8 3 Shortwave Infrared: 12 8 4 Vegetation Analysis: 11 8 4","title":"RGB colour composites 2"},{"location":"P3/P3-colour-composites-ctd/#displaying-more-band-combinations","text":"When you have performed an image resampling, open again the RGB image window in SNAP. You will notice that the list with possible band combinations is larger. Test some of the following band combinations and explore the colour differences. Which features are most clear on the following band combinations? Natural Colours: 4 3 2 False colour Infrared: 8 4 3 False colour Urban: 12 11 4 Agriculture: 11 8 2 Atmospheric penetration: 12 11 8a Healthy vegetation: 8 11 2 Land/Water: 8 11 4 Natural Colours with Atmospheric Removal: 12 8 3 Shortwave Infrared: 12 8 4 Vegetation Analysis: 11 8 4","title":"Displaying more band combinations"},{"location":"P3/P3-colour-composites/","text":"About colour composites \u00b6 Multispectral imagery, such as Sentinel-2, consists of several bands of data. As seen during in previous chapter, these bands can be displayed individually as a grey scale image (black = low reflectance, white = high reflectance), but they can also be displayed as a combination of three bands: a colour composite (NL: kleurcomposiet). When creating a colour composite: the three primary colours are used: red, green and blue. When they are combined in various proportions, different colours are produced per pixel. When 3 spectral bands (both visible as non-visible bands) are assigned to a primary colour, a colour composite is formed. By combining different proportions of the three primary colours Red, Green and Blue, various colours are created Two \"famous\" colour composites \u00b6 True Colour Composite \u00b6 The most straightforward colour composite is the true colour composite (also natural colour composite ), where the three visual primary colour bands of a multispectral image are assigned to their corresponding colour. For Sentinel 2, this composite is created as: Red: B4, Green: B3, Blue: B2. Sentinel-2 Normal Composite of Ghent. False Colour Composite \u00b6 Beside the 'normal' colour composites, any band of a multispectral satellite image can be assigned to the primary colour bands in a composite. In all those other cases, the colour of a target object on the image, will have a different colour compared to it's actual colour. The most famous of these is the False Colour Composite , where the NIR-band is assigned to the red colour, the red band to the green colour and the green band to the blue colour. It is very suitable to detect vegetation, since vegetation has a high reflectance in the NIR band. Clear water will appear dark-bluish, while turbid water (with a lot of sediments) will be cyan. Bare soils, roads and buildings may appear in various shades of blue, yellow or grey, depending on their composition. For Sentinel 2, this composite is created as: Red: B8, Green: B4, Blue: B3. Sentinel-2 False Colour Composite of Ghent. Opening a RGB image in SNAP \u00b6 Let's create our own image composites in SNAP! This is actually very easy to do. Just right-click on the image folder and click on 'Open RGB Image window': A window will appear with some possible S2 band combinations, but you can also create your own. Some typical S2 band combinations have their own name, such as (Red, Green, Blue): Natural Colour: 4 3 2 * False colour Infrared: 8 4 3 * False colour Urban: 12 11 4 Agriculture: 11 8 2 Atmospheric penetration: 12 11 8a * Healthy vegetation: 8 11 2 Land/Water: 8 11 4 Natural Colours with Atmospheric Removal: 12 8 3 Shortwave Infrared: 12 8 4 Vegetation Analysis: 11 8 4 With the current Sentinel-2 Level 1C-product open, only the band combinations with a * can now be displayed. Why is that? Excercise: open band composites Open the image as a natural colour composite Open the image as a false colour infrared composite Tile the images evenly and explore the difference in colour (for example in the areas with green vegetation).","title":"RGB colour composites"},{"location":"P3/P3-colour-composites/#about-colour-composites","text":"Multispectral imagery, such as Sentinel-2, consists of several bands of data. As seen during in previous chapter, these bands can be displayed individually as a grey scale image (black = low reflectance, white = high reflectance), but they can also be displayed as a combination of three bands: a colour composite (NL: kleurcomposiet). When creating a colour composite: the three primary colours are used: red, green and blue. When they are combined in various proportions, different colours are produced per pixel. When 3 spectral bands (both visible as non-visible bands) are assigned to a primary colour, a colour composite is formed. By combining different proportions of the three primary colours Red, Green and Blue, various colours are created","title":"About colour composites"},{"location":"P3/P3-colour-composites/#two-famous-colour-composites","text":"","title":"Two \"famous\" colour composites"},{"location":"P3/P3-colour-composites/#true-colour-composite","text":"The most straightforward colour composite is the true colour composite (also natural colour composite ), where the three visual primary colour bands of a multispectral image are assigned to their corresponding colour. For Sentinel 2, this composite is created as: Red: B4, Green: B3, Blue: B2. Sentinel-2 Normal Composite of Ghent.","title":"True Colour Composite"},{"location":"P3/P3-colour-composites/#false-colour-composite","text":"Beside the 'normal' colour composites, any band of a multispectral satellite image can be assigned to the primary colour bands in a composite. In all those other cases, the colour of a target object on the image, will have a different colour compared to it's actual colour. The most famous of these is the False Colour Composite , where the NIR-band is assigned to the red colour, the red band to the green colour and the green band to the blue colour. It is very suitable to detect vegetation, since vegetation has a high reflectance in the NIR band. Clear water will appear dark-bluish, while turbid water (with a lot of sediments) will be cyan. Bare soils, roads and buildings may appear in various shades of blue, yellow or grey, depending on their composition. For Sentinel 2, this composite is created as: Red: B8, Green: B4, Blue: B3. Sentinel-2 False Colour Composite of Ghent.","title":"False Colour Composite"},{"location":"P3/P3-colour-composites/#opening-a-rgb-image-in-snap","text":"Let's create our own image composites in SNAP! This is actually very easy to do. Just right-click on the image folder and click on 'Open RGB Image window': A window will appear with some possible S2 band combinations, but you can also create your own. Some typical S2 band combinations have their own name, such as (Red, Green, Blue): Natural Colour: 4 3 2 * False colour Infrared: 8 4 3 * False colour Urban: 12 11 4 Agriculture: 11 8 2 Atmospheric penetration: 12 11 8a * Healthy vegetation: 8 11 2 Land/Water: 8 11 4 Natural Colours with Atmospheric Removal: 12 8 3 Shortwave Infrared: 12 8 4 Vegetation Analysis: 11 8 4 With the current Sentinel-2 Level 1C-product open, only the band combinations with a * can now be displayed. Why is that? Excercise: open band composites Open the image as a natural colour composite Open the image as a false colour infrared composite Tile the images evenly and explore the difference in colour (for example in the areas with green vegetation).","title":"Opening a RGB image in SNAP"},{"location":"P3/P3-image-preprocessing/","text":"Radiometric & atmospheric correction \u00b6 Satellite images obtained by the sensing device are not directly usable. They need to go through a series of pre-processing before they are ready to use. The scheme below illustrates the pre-processing steps that Sentinel-images undergo before they are made available for the user. This includes geometric correction, some radiometric correction (noise reduction, defective pixels identification) the computation of cloud masks, etc. The outcome is a level 1C product, which is Top-Of-the-Atmosphere (TOA). TOA reflectances are subjected to radiometric bias caused by different lighting conditions, atmospheric interactions and viewing geometry. In order to relate reflectances to physical field properties, TOA reflectance values are conversed to BOA (Bottom Of Atmosphere) corrected reflectance values. This radiometric correction is an essential part in image processing. BOA, Sentinel processing level 2A, is available for the user (except for recent images) or can be created by the user itself, using the Sen2Cor freeware. Figure: A true color comparison of the surface reflectance product (top) and a top of atmosphere reflectance image (bottom) in adjacent scenes captured by the same satellite (Planet.com) In Snap, the conversion of level 1C TOA-reflectance to level 2A BOA-reflectance can be done through Sen2Cor (plug-in or stand-alone). Sen2Cor corrects the reflectance values based on (among others) \u2018look-up tables\u2019, these are tables that relate physical parameters to model coefficients. Parameters such as inclination and product type are sensor dependent (different for Landsat as for Sentinel or Spot). On board, optical satellites have some meteorological sensors that measure atmosphere features such as the air thickness and the amount of aerosols among others. This information is available as a \u2018header file\u2019 for each image. Since December 2018, users can download Level-2A processed products directly. In case of this exercise, we downloaded a Level 1C product. Thus, let\u2019s perform an atmospheric correction! Excercise: atmospheric correction with Sen2Cor In the folder where you have saved the image, unzip the Sentinel-image. Go to \u2018Optical\u2019 > \u2018Thematic Land Processing\u2019 > \u2018Sen2Cor processor\u2019 > \u2018Sen2Cor280\u2019 When you choose the source product, click on the \u2018\u2026\u2019, browse to the image and navigate to the \u2018MTD_MSIL1C.xml\u2019 product. In the tab \u2018processing parameters\u2019, set the resolution to \u2018ALL\u2019 The other processing parameters are by default taken from a combination of the image metadata (header file) and look-up tables. This is why you will normally use the default processing parameters. However, if you want to adjust these parameters, you can do that manually. Run Sen2Cor (!be patient, it will take a while to process the entire image.) Explore the outcome image (RGB). What differences do you see according to the original image? Installing 'Sen2Cor' plugin Possibly sen2cor isn\u2019t installed yet. To do this, go to \u2018Tools\u2019 > \u2018Plugins\u2019. During the first run, you\u2019ll get an error, after which an extra bundle will be installed. ). Intermezzo: Cloud Masks \u00b6 The image contain clouds. This means that there are some blind pixels, which lack information on the reflectance of the earth\u2019s surface at the sensing time. This phenomenon is very common in tropical areas with a rainy season. It is possible that over the whole period of the rainy season, you will not be able to obtain images with a cloud cover of less than 90%. In such cases, Radar imaging can be useful, but are complexer. An introduction to radar imaging will be given later in these practicals. Included in a Sentinel-2 image folder you can find some cloud masks at a resolution of 10m, 20m and 60m. These cloud masks enable the user to identify cloudy and cloud-free pixels. The masks include both dense clouds (opaque clouds) and cirrus clouds. These cloud masks are computed by a threshold algorithm. Below, the methods are described that identify the cloud pixels (for your information). Identification of dense clouds Dense clouds, also called opaque clouds, are characterised by a high reflectance in the blue spectral region (B2). The method used to identify dense cloud pixels is based on B2 reflectance threshold. To avoid false detection, mainly due to snow/cloud confusion, SWIR reflectance in B11 and B12 are also used. Snow and clouds both have a high reflectance in the blue. Cloud reflectance is high in the SWIR, whereas snow presents a low reflectance. Additional criteria based on B10 reflectance are added to avoid high altitude ice cloud and snow confusion (both having a low reflectance in the SWIR bands B11 and B12). At B10, there is a high atmospheric absorption band and only high altitude clouds are detected. However, this last criterion is only applied after a first detection of cloud pixel in the blue band where cirrus is transparent. Identification of cirrus clouds Cirrus clouds are thin, transparent or semi-transparent clouds, forming at high altitudes, approximately 6-7 km above the Earth's surface. The method of identifying cirrus cloud pixels from dense cloud pixel is based on two spectral criteria: (1) B10 corresponds to a high atmospheric absorption band: only high altitude clouds can be detected, (2) cirrus clouds, being semi-transparent, cannot be detected in the B2 blue band. A pixel with low reflectance in the B2 band and high reflectance in the B10 band has a good probability of being cirrus cloud but this is not a certainty. Some opaque clouds have a low reflectance in the blue and can be identified as cirrus cloud. To limit false detections (due to high reflectance in the blue or due to the fact that clouds are not spectrally registered), a filter using morphology-based operations is applied on both dense and cirrus cloud masks: (1) erosion, to remove isolated pixels, (2) dilatation, to fill the gap and extend clouds. If after morphology operations, a pixel is both dense and cirrus, the dense cloud mask prevails. Sen2Cor scene classification The Sen2Cor-processor you've runned for the atmospheric correction from the level 1C to the level 2A product also contains a scene classification algorithm. This algorithm creates a scene classification, where pixels als classified in some broad classes: Here, clouds are classified into 'cloud probability masks', which are in general more precise than the level 1C cloud masks. Excercise: Visualize cloud masks Visualize the cloud masks. If you look at the cloud masks, you will see that these are not very precise. These cloud masks are useful for rough estimations. Later we will see alternative ways to identify cloud pixels more precise. Resampling \u00b6 In order to display the other band combinations, some geometrical pre-processing is necessary. The bands have to be resampled to an equal resolution. The goal is to resample the image bands to 10m (you can take B2, B3, B4 or B8 as a reference band). This means that all other bands will be upsampled. Image resampling scheme. Top: upsampling (nearest neighbor). Bottom: Downsampling (minimum). Exercise: resampling In the product explorer, select the outcome image of Sen2Cor. Go to Raster > Geometric operations > Resampling . Select the \u2018Save as\u2026 BEAM-DIMAP\u2019 box. Browse to your directory. Choose a logical name for the target product. Resampling Parameters: Choose a reference band that has a resolution of 10m, or choose for a pixel resolution of 10m. Use an upsampling method of your choice (Read the help for more details on the different algorithms). Run resampling . Saving the images takes a lot of time. Again, be patient! Image Subsetting \u00b6 Processing an entire Sentinel image takes a lot of processing capacity and time (as you probably have noted already). Therefore, you will now learn how to only process a small part of the image. You can choose to reduce the spatial extent of the image, or you can choose to reduce the amount of bands in the image, or a combination of both. An important aspect is that creating a subset is only possible for bands that have the same size. Thus, this will only be possible after resampling . Excercise: subsetting an image Select the resampled image in the product explorer. Go to Raster > Subset. Select a spatial subset by choice (by adjusting the scene start and end). Make sure your spatial extent is substantially smaller than the original image. Snap Subsetting screen. Select only following bands: [B2, B3, B4, B5, B6, B7, B8, B8A, B11, B12] You can see an estimation of the new required storage space. Snap Subsetting screen. Click OK Another option to make a subset is \u2018Spatial subset from view\u2019. Zoom in on your image. Rightclick and select \u2018Spatial subset from view\u2019. FYI: it is also possible to take a subset of an image, based on a vector layer. Mosaicing \u00b6 Mosaicing is the merging of several arbitrarily shaped images and often used to merge two neighbouring satellite images. Excercise: mosaicing Download an image that is located next to the image you are already working with, dating from the same time as the original image was taken. You can download it directly in Level 2A, thus skipping the sen2cor atmospheric correction. Resample the image. Go to raster > Geometric operations > Mosaicing Snap mosaicing screen. Add the two source products. Choose the directory in which you want to save the mosaic image. In the Map Projection Definition you can choose the Coordinate Reference System (CRS). Choose for UTM/WGS84 (automatic) Choose for a resolution of 10m. The input products don\u2019t need to be orthorectified (because they already are). In the tab \u2018Variables and Conditions\u2019, click the - symbol. Select Band 2,3,4 and 8 Run Mosaicing. Open the RGB-image of the product. Compare it to the two original images. Extra: Examine the example of Landsat satellite image after merging below. What went wrong when mosaicing images 1 and 2? Why is there a colour difference in 2 and 3? Why is there no observable colour difference in 2 and 4? Have you any idea how to eliminate the colour difference between 2 and 3, given that neighbouring satellite images always partly overlap? Landsat images mosaic","title":"Image preprocessing in SNAP"},{"location":"P3/P3-image-preprocessing/#radiometric-atmospheric-correction","text":"Satellite images obtained by the sensing device are not directly usable. They need to go through a series of pre-processing before they are ready to use. The scheme below illustrates the pre-processing steps that Sentinel-images undergo before they are made available for the user. This includes geometric correction, some radiometric correction (noise reduction, defective pixels identification) the computation of cloud masks, etc. The outcome is a level 1C product, which is Top-Of-the-Atmosphere (TOA). TOA reflectances are subjected to radiometric bias caused by different lighting conditions, atmospheric interactions and viewing geometry. In order to relate reflectances to physical field properties, TOA reflectance values are conversed to BOA (Bottom Of Atmosphere) corrected reflectance values. This radiometric correction is an essential part in image processing. BOA, Sentinel processing level 2A, is available for the user (except for recent images) or can be created by the user itself, using the Sen2Cor freeware. Figure: A true color comparison of the surface reflectance product (top) and a top of atmosphere reflectance image (bottom) in adjacent scenes captured by the same satellite (Planet.com) In Snap, the conversion of level 1C TOA-reflectance to level 2A BOA-reflectance can be done through Sen2Cor (plug-in or stand-alone). Sen2Cor corrects the reflectance values based on (among others) \u2018look-up tables\u2019, these are tables that relate physical parameters to model coefficients. Parameters such as inclination and product type are sensor dependent (different for Landsat as for Sentinel or Spot). On board, optical satellites have some meteorological sensors that measure atmosphere features such as the air thickness and the amount of aerosols among others. This information is available as a \u2018header file\u2019 for each image. Since December 2018, users can download Level-2A processed products directly. In case of this exercise, we downloaded a Level 1C product. Thus, let\u2019s perform an atmospheric correction! Excercise: atmospheric correction with Sen2Cor In the folder where you have saved the image, unzip the Sentinel-image. Go to \u2018Optical\u2019 > \u2018Thematic Land Processing\u2019 > \u2018Sen2Cor processor\u2019 > \u2018Sen2Cor280\u2019 When you choose the source product, click on the \u2018\u2026\u2019, browse to the image and navigate to the \u2018MTD_MSIL1C.xml\u2019 product. In the tab \u2018processing parameters\u2019, set the resolution to \u2018ALL\u2019 The other processing parameters are by default taken from a combination of the image metadata (header file) and look-up tables. This is why you will normally use the default processing parameters. However, if you want to adjust these parameters, you can do that manually. Run Sen2Cor (!be patient, it will take a while to process the entire image.) Explore the outcome image (RGB). What differences do you see according to the original image? Installing 'Sen2Cor' plugin Possibly sen2cor isn\u2019t installed yet. To do this, go to \u2018Tools\u2019 > \u2018Plugins\u2019. During the first run, you\u2019ll get an error, after which an extra bundle will be installed. ).","title":"Radiometric &amp; atmospheric correction"},{"location":"P3/P3-image-preprocessing/#intermezzo-cloud-masks","text":"The image contain clouds. This means that there are some blind pixels, which lack information on the reflectance of the earth\u2019s surface at the sensing time. This phenomenon is very common in tropical areas with a rainy season. It is possible that over the whole period of the rainy season, you will not be able to obtain images with a cloud cover of less than 90%. In such cases, Radar imaging can be useful, but are complexer. An introduction to radar imaging will be given later in these practicals. Included in a Sentinel-2 image folder you can find some cloud masks at a resolution of 10m, 20m and 60m. These cloud masks enable the user to identify cloudy and cloud-free pixels. The masks include both dense clouds (opaque clouds) and cirrus clouds. These cloud masks are computed by a threshold algorithm. Below, the methods are described that identify the cloud pixels (for your information). Identification of dense clouds Dense clouds, also called opaque clouds, are characterised by a high reflectance in the blue spectral region (B2). The method used to identify dense cloud pixels is based on B2 reflectance threshold. To avoid false detection, mainly due to snow/cloud confusion, SWIR reflectance in B11 and B12 are also used. Snow and clouds both have a high reflectance in the blue. Cloud reflectance is high in the SWIR, whereas snow presents a low reflectance. Additional criteria based on B10 reflectance are added to avoid high altitude ice cloud and snow confusion (both having a low reflectance in the SWIR bands B11 and B12). At B10, there is a high atmospheric absorption band and only high altitude clouds are detected. However, this last criterion is only applied after a first detection of cloud pixel in the blue band where cirrus is transparent. Identification of cirrus clouds Cirrus clouds are thin, transparent or semi-transparent clouds, forming at high altitudes, approximately 6-7 km above the Earth's surface. The method of identifying cirrus cloud pixels from dense cloud pixel is based on two spectral criteria: (1) B10 corresponds to a high atmospheric absorption band: only high altitude clouds can be detected, (2) cirrus clouds, being semi-transparent, cannot be detected in the B2 blue band. A pixel with low reflectance in the B2 band and high reflectance in the B10 band has a good probability of being cirrus cloud but this is not a certainty. Some opaque clouds have a low reflectance in the blue and can be identified as cirrus cloud. To limit false detections (due to high reflectance in the blue or due to the fact that clouds are not spectrally registered), a filter using morphology-based operations is applied on both dense and cirrus cloud masks: (1) erosion, to remove isolated pixels, (2) dilatation, to fill the gap and extend clouds. If after morphology operations, a pixel is both dense and cirrus, the dense cloud mask prevails. Sen2Cor scene classification The Sen2Cor-processor you've runned for the atmospheric correction from the level 1C to the level 2A product also contains a scene classification algorithm. This algorithm creates a scene classification, where pixels als classified in some broad classes: Here, clouds are classified into 'cloud probability masks', which are in general more precise than the level 1C cloud masks. Excercise: Visualize cloud masks Visualize the cloud masks. If you look at the cloud masks, you will see that these are not very precise. These cloud masks are useful for rough estimations. Later we will see alternative ways to identify cloud pixels more precise.","title":"Intermezzo: Cloud Masks"},{"location":"P3/P3-image-preprocessing/#resampling","text":"In order to display the other band combinations, some geometrical pre-processing is necessary. The bands have to be resampled to an equal resolution. The goal is to resample the image bands to 10m (you can take B2, B3, B4 or B8 as a reference band). This means that all other bands will be upsampled. Image resampling scheme. Top: upsampling (nearest neighbor). Bottom: Downsampling (minimum). Exercise: resampling In the product explorer, select the outcome image of Sen2Cor. Go to Raster > Geometric operations > Resampling . Select the \u2018Save as\u2026 BEAM-DIMAP\u2019 box. Browse to your directory. Choose a logical name for the target product. Resampling Parameters: Choose a reference band that has a resolution of 10m, or choose for a pixel resolution of 10m. Use an upsampling method of your choice (Read the help for more details on the different algorithms). Run resampling . Saving the images takes a lot of time. Again, be patient!","title":"Resampling"},{"location":"P3/P3-image-preprocessing/#image-subsetting","text":"Processing an entire Sentinel image takes a lot of processing capacity and time (as you probably have noted already). Therefore, you will now learn how to only process a small part of the image. You can choose to reduce the spatial extent of the image, or you can choose to reduce the amount of bands in the image, or a combination of both. An important aspect is that creating a subset is only possible for bands that have the same size. Thus, this will only be possible after resampling . Excercise: subsetting an image Select the resampled image in the product explorer. Go to Raster > Subset. Select a spatial subset by choice (by adjusting the scene start and end). Make sure your spatial extent is substantially smaller than the original image. Snap Subsetting screen. Select only following bands: [B2, B3, B4, B5, B6, B7, B8, B8A, B11, B12] You can see an estimation of the new required storage space. Snap Subsetting screen. Click OK Another option to make a subset is \u2018Spatial subset from view\u2019. Zoom in on your image. Rightclick and select \u2018Spatial subset from view\u2019. FYI: it is also possible to take a subset of an image, based on a vector layer.","title":"Image Subsetting"},{"location":"P3/P3-image-preprocessing/#mosaicing","text":"Mosaicing is the merging of several arbitrarily shaped images and often used to merge two neighbouring satellite images. Excercise: mosaicing Download an image that is located next to the image you are already working with, dating from the same time as the original image was taken. You can download it directly in Level 2A, thus skipping the sen2cor atmospheric correction. Resample the image. Go to raster > Geometric operations > Mosaicing Snap mosaicing screen. Add the two source products. Choose the directory in which you want to save the mosaic image. In the Map Projection Definition you can choose the Coordinate Reference System (CRS). Choose for UTM/WGS84 (automatic) Choose for a resolution of 10m. The input products don\u2019t need to be orthorectified (because they already are). In the tab \u2018Variables and Conditions\u2019, click the - symbol. Select Band 2,3,4 and 8 Run Mosaicing. Open the RGB-image of the product. Compare it to the two original images. Extra: Examine the example of Landsat satellite image after merging below. What went wrong when mosaicing images 1 and 2? Why is there a colour difference in 2 and 3? Why is there no observable colour difference in 2 and 4? Have you any idea how to eliminate the colour difference between 2 and 3, given that neighbouring satellite images always partly overlap? Landsat images mosaic","title":"Mosaicing"},{"location":"P3/P3-intro/","text":"Practicum 3: Image download & preprocessing \u00b6 Doel van het practicum \u00b6 Downloaden van remote sensing data: via ESA sentinel hub via andere bronnen Introductie tot ESA SNAP: Inlezen van RS beelden Basisfunctionaliteiten Aanmaken van beeldcomposieten SNAP vs andere software Beeldvoorbewerking in SNAP (Sentinel 2): Radiometrische/atmospherische correctie Resampling Subsetting Mosaicing","title":"Introduction"},{"location":"P3/P3-intro/#practicum-3-image-download-preprocessing","text":"","title":"Practicum 3: Image download &amp; preprocessing"},{"location":"P3/P3-intro/#doel-van-het-practicum","text":"Downloaden van remote sensing data: via ESA sentinel hub via andere bronnen Introductie tot ESA SNAP: Inlezen van RS beelden Basisfunctionaliteiten Aanmaken van beeldcomposieten SNAP vs andere software Beeldvoorbewerking in SNAP (Sentinel 2): Radiometrische/atmospherische correctie Resampling Subsetting Mosaicing","title":"Doel van het practicum"}]}